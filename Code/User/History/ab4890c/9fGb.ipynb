{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation and jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import jax\n",
    "from jax import numpy as np\n",
    "from jax import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed():\n",
    "    return jax.random.PRNGKey(random.randint(0, 228))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_differentiation_methods(f, gradf, shape, repeat_times=5):\n",
    "    for i in range(repeat_times):\n",
    "        x = jax.random.uniform(seed(), shape=shape)\n",
    "        isclose = np.isclose(gradf(x), jax.grad(f)(x)).flatten()\n",
    "\n",
    "        print(f\"Iteration {i}: \", end='')\n",
    "        if np.all(isclose == True):\n",
    "            print(\"all components are close\")\n",
    "        else:\n",
    "            print(\"some components differ\")\n",
    "            print(f\"Max componentwise relative error is: {((gradf(x) - jax.grad(f)(x)) / gradf(x)).max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return np.exp(-(np.sin(x) - np.cos(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = jax.xla_computation(f)(np.ones(1337), np.ones(1337))\n",
    "with open(\"graph.dot\", \"w\") as file:\n",
    "    file.write(graph.as_hlo_dot_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336.80s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "!dot graph.dot -Tpng > graph.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(A) = \\operatorname{tr}(e^A),\\, A \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "$ \\nabla f(A) = \\exp(A^{\\top}) $ proved in task 4 in Matrix calculus hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(A):\n",
    "    return np.trace(sp.linalg.expm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradf(A):\n",
    "    return sp.linalg.expm(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: some components differ\n",
      "Max componentwise relative error is: -6.508145452244207e-05\n",
      "Iteration 1: some components differ\n",
      "Max componentwise relative error is: -9.928335202857852e-06\n",
      "Iteration 2: some components differ\n",
      "Max componentwise relative error is: -4.052296208101325e-05\n",
      "Iteration 3: some components differ\n",
      "Max componentwise relative error is: -3.57690078089945e-05\n",
      "Iteration 4: some components differ\n",
      "Max componentwise relative error is: -1.8703463865676895e-05\n"
     ]
    }
   ],
   "source": [
    "compare_differentiation_methods(f, gradf, (20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, different approaches give us different results with maximum relative error about $10^{-5}$ or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(x) = \\frac{1}{2} \\| x \\|^2,\\, x \\in \\mathbb{R}^n $\n",
    "\n",
    "$ f(x) = \\frac{1}{2} \\langle x,\\, x \\rangle $\n",
    "\n",
    "$ df(x) = \\langle x,\\, dx \\rangle $\n",
    "\n",
    "$ \\nabla f(x) = x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(x_0):\n",
    "    def wrapper(alpha):\n",
    "        nonlocal x_0\n",
    "        x = x_0\n",
    "        for i in range(10):\n",
    "            x = x - alpha[i] * x\n",
    "        return np.linalg.norm(x) / 2\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = float(jax.random.uniform(seed(), shape=(1,))[0])\n",
    "alpha_1 = jax.random.uniform(seed(), maxval=0.1, shape=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "safe_zip() argument 2 is shorter than argument 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m alpha_2 \u001b[39m=\u001b[39m alpha_1\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     alpha_2\u001b[39m.\u001b[39;49mat[i]\u001b[39m.\u001b[39;49mset(alpha_2[i \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39;49m beta \u001b[39m*\u001b[39;49m jax\u001b[39m.\u001b[39;49mgrad(L(x_0))(alpha_2))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:482\u001b[0m, in \u001b[0;36m_IndexUpdateRef.set\u001b[0;34m(self, values, indices_are_sorted, unique_indices, mode)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m, values, \u001b[39m*\u001b[39m, indices_are_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, unique_indices\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    474\u001b[0m         mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    475\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Pure equivalent of ``x[idx] = y``.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \n\u001b[1;32m    477\u001b[0m \u001b[39m  Returns the value of ``x`` that would result from the NumPy-style\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39m  See :mod:`jax.ops` for details.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m   \u001b[39mreturn\u001b[39;00m scatter\u001b[39m.\u001b[39;49m_scatter_update(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, values, lax\u001b[39m.\u001b[39;49mscatter,\n\u001b[1;32m    483\u001b[0m                                  indices_are_sorted\u001b[39m=\u001b[39;49mindices_are_sorted,\n\u001b[1;32m    484\u001b[0m                                  unique_indices\u001b[39m=\u001b[39;49munique_indices, mode\u001b[39m=\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/jax/_src/ops/scatter.py:78\u001b[0m, in \u001b[0;36m_scatter_update\u001b[0;34m(x, idx, y, scatter_op, indices_are_sorted, unique_indices, mode, normalize_indices)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m# XLA gathers and scatters are very similar in structure; the scatter logic\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# is more or less a transpose of the gather equivalent.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39m_split_index_for_jit(idx, x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,\n\u001b[1;32m     79\u001b[0m                      indices_are_sorted, unique_indices, mode,\n\u001b[1;32m     80\u001b[0m                      normalize_indices)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/jax/_src/ops/scatter.py:112\u001b[0m, in \u001b[0;36m_scatter_impl\u001b[0;34m(x, y, scatter_op, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, normalize_indices)\u001b[0m\n\u001b[1;32m    109\u001b[0m x, y \u001b[39m=\u001b[39m promote_dtypes(x, y)\n\u001b[1;32m    111\u001b[0m \u001b[39m# Broadcast `y` to the slice output shape.\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49mbroadcast_to(y, \u001b[39mtuple\u001b[39;49m(indexer\u001b[39m.\u001b[39;49mslice_shape))\n\u001b[1;32m    113\u001b[0m \u001b[39m# Collapse any `None`/`jnp.newaxis` dimensions.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msqueeze(y, axis\u001b[39m=\u001b[39mindexer\u001b[39m.\u001b[39mnewaxis_dims)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:1194\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m@util\u001b[39m\u001b[39m.\u001b[39m_wraps(np\u001b[39m.\u001b[39mbroadcast_to, lax_description\u001b[39m=\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39mThe JAX version does not necessarily return a view of the input.\u001b[39m\n\u001b[1;32m   1192\u001b[0m \u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m   1193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbroadcast_to\u001b[39m(array: ArrayLike, shape: Shape) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[0;32m-> 1194\u001b[0m   \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49m_broadcast_to(array, shape)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/jax/_src/numpy/util.py:399\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[0;34m(arr, shape)\u001b[0m\n\u001b[1;32m    396\u001b[0m nlead \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(shape) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(arr_shape)\n\u001b[1;32m    397\u001b[0m shape_tail \u001b[39m=\u001b[39m shape[nlead:]\n\u001b[1;32m    398\u001b[0m compatible \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m(core\u001b[39m.\u001b[39mdefinitely_equal_one_of_dim(arr_d, [\u001b[39m1\u001b[39m, shape_d])\n\u001b[0;32m--> 399\u001b[0m                  \u001b[39mfor\u001b[39;00m arr_d, shape_d \u001b[39min\u001b[39;00m safe_zip(arr_shape, shape_tail))\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m nlead \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m compatible:\n\u001b[1;32m    401\u001b[0m   msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and requested shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: safe_zip() argument 2 is shorter than argument 1"
     ]
    }
   ],
   "source": [
    "beta = 0.1\n",
    "\n",
    "alpha_2 = alpha_1\n",
    "for i in range(10):\n",
    "    alpha_2.at[i].set(alpha_2[i - 1] - beta * jax.grad(L(x_0))(alpha_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.04536309, dtype=float32)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(x_0)(alpha_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(x) = -\\log \\det X,\\, X \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "$ df(x) = -\\frac{\\det X \\cdot \\langle X^{-\\top},\\, dX \\rangle}{\\det X} = \\langle X^{-\\top},\\, dX \\rangle$\n",
    "\n",
    "$ \\nabla f(x) = -X^{-\\top} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return -np.log(np.linalg.det(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradf(X):\n",
    "    return -np.linalg.inv(X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: some components differ\n",
      "Max componentwise relative error is: 0.00013778700667899102\n",
      "Iteration 1: some components differ\n",
      "Max componentwise relative error is: 9.875793330138549e-05\n",
      "Iteration 2: some components differ\n",
      "Max componentwise relative error is: 5.780803348898189e-06\n",
      "Iteration 3: some components differ\n",
      "Max componentwise relative error is: 1.0148980436497368e-05\n",
      "Iteration 4: some components differ\n",
      "Max componentwise relative error is: 0.00010453537106513977\n"
     ]
    }
   ],
   "source": [
    "compare_differentiation_methods(f, gradf, (20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, different approaches give us different results with maximum relative error about $10^{-4}$ or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(x) = x^{\\top} x x^{\\top} x,\\, x \\in \\mathbb{R}^n $\n",
    "\n",
    "$ f(x) = \\langle x,\\, x \\rangle^2 $\n",
    "\n",
    "$ df(x) = 4 \\cdot \\langle x,\\, x \\rangle \\cdot \\langle x,\\, dx \\rangle = \\big\\langle 4 \\cdot \\langle x,\\, x \\rangle \\cdot x,\\, dx \\big\\rangle $\n",
    "\n",
    "$ \\nabla f(x) = 4 \\cdot \\langle x,\\, x \\rangle \\cdot x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x.T @ x) * (x.T @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradf(x):\n",
    "    return 4 * (x.T @ x) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: all components are close\n",
      "Iteration 1: all components are close\n",
      "Iteration 2: all components are close\n",
      "Iteration 3: all components are close\n",
      "Iteration 4: all components are close\n"
     ]
    }
   ],
   "source": [
    "compare_differentiation_methods(f, gradf, (100,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
